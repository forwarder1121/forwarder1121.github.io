---
title: "CS231n 03강 Loss Functions and Optimization"
excerpt: "Kuggle 10th Study"
categories:
    - CS231n
---





저번 강의에서는 선형 분류에 대해서 배웠다.

이는 가중치 행렬 W를 Wx+b로 행렬곱하여 분류 결과를 얻는 방식이다.

여기서 ML에서 중요한 점이

훈련을 통하여 W 행렬의 값을 계속 나아지게 만들어야 하는데, 그러기 위해서는 **W가 얼마나 좋은가**의 측도가 필요하다는 것이다.

이를 위하여 Loss Function이라는 함수를 만들어, 행렬 W가 얼마나 나쁜지 판단하여 가중치 행렬 W 업데이트에 이용한다.



---



# Loss Function



손실함수에는 2가지 종류가 있다.

Softmax(cross-entropy loss), SVM Loss가 존재한다.

이 손실함수들은 다음과 같이 정의된다.



![image](https://github.com/user-attachments/assets/5dec5952-c652-473d-9504-859c888a2dc5)



이 손실함수의 값을 최소화하는 방향으로 가중치 행렬 W를 갱신시켜 나가면 Machine을 훈련시킬 수 있다. 그러나, 훈련 데이터에 대하여 지나치게 의존하는 과적합 현상이 발생할 수 있기 때문에, 규제를 사용한다.



## Regularization

규제의 종류에는 대표적으로 L1, L2 두 가지와 이 둘을 섞은 Elastic net이 있다.



![image](https://github.com/user-attachments/assets/ad1555ab-1f02-412d-a33e-44d5c9734cc8)



추가로 과적합을 막기 위하여 Dropout Batch normalization, Stochastic depth, fractional pooling 등이 추가로 존재하나 이는 다음에 다룬다.



L2 규제를 사용하게 되면, 가중치 행렬을 거듭제곱하는 성질 덕분에 큰 성분이 있다면 거듭제곱으로 인하여 더 커질 것이다. 따라서 L2 규제는 가중치 행렬의 weights를 최대한 균일하게 **spread out** 시키는 방향으로 W를 업데이트 한다.



반면 L1 규제는 이와 반대로 가중치 행렬의 한 성분을 크게 만들고 나머지는 0으로 만드는 성질이 있다.



따라서 Full loss는 아래와 같이 나타낼 수 있다.

![image](https://github.com/user-attachments/assets/c4e1087b-da31-4de2-95ff-8535df8cd039)



---



# Optimization



이제 우리는 가중치 행렬 W의 나쁜 정도의 측도인 **Loss function**를 정의하였다.

따라서 우리는 **Loss function의 값을 최소화 하는 방향으로 W를 optimization** 해주기만 하면 된다.

그러면 어떻게 이를 수행할 수 있을까?

강의에서는 2가지 방법을 제시한다.



## Random search

이 방법은 W의 값을 랜덤하게 1000회 설정하여 그 중 가장 최소의 손실함수를 가지는 W를 설정하는 것을 나타낸다.

그러나 이는 매우 나쁜 접근법이다. 

수학적이지 않고, 따라서 최적의 W를 찾기 위하여 엄청난 계산량을 요구할 것이다.

![image](https://github.com/user-attachments/assets/d835192d-95cc-4c12-8979-0ebb7ab3f2bc)

이는 0.15의 정확도를 보인다.



## Follow the slope

미분을 활용하여 Loss function의 값이 가장 최소가 되는 방향을 구하여 그 부분으로 W를 업데이트 하는 방법이다. 

이때 미분을 하는 방법도 2가지가 있는데

1. Numeric Gradient

   미분계수의 공식 f(W+h)-f(W)을 계산할때 h에 임의의 작은 수를 대입하여 구하는 수치미분 방식. 그러나 이는 너무 느리고, 근삿값만 구할 수 있다는 점에서 한계가 존재한다.

2. Analytic Gradient

   수학적으로 정확한 기울기를 구한다. 정확하고 빠르기 때문에 실제로는 이를 이용한다. 다만 오류 발생가능성이 있기 때문에 수치미분으로 이 값을 확인한다.



이렇게 구한 기울기를 바탕으로 Gradient Descent를 수행한다.

![image](https://github.com/user-attachments/assets/52ce10fb-9d9a-4dee-886a-49cf2bc904f5)

위 코드가 이를 나타내는데, 쉽게 말해

**Loss function의 값이 가장 최소가 되는 방향으로 W를 업데이트 하는 로직**이다.



## Stochastic Gradient Descent (SGD)

그리고, 데이터의 개수인 N이 클 때는 Loss function을 계산하는 것조차 많은 연산량을 요구 한다. 따라서 확률적으로 전체 데이터에서 일부 샘플을 뽑아 이를 통하여 Loss function을 계산하는 Stochastic Gradient Descent(SGD) 방법을 주로 사용한다.



![image](https://github.com/user-attachments/assets/3b3c8730-9781-480d-8f6e-f9f782ec8742)



